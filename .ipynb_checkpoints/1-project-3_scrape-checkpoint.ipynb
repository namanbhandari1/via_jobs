{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/depression.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/depression.json\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bufbr\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bsip2\n",
      "https://www.reddit.com/r/depression.json?after=t3_9br1xk\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bx1j0\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bw2p8\n",
      "https://www.reddit.com/r/depression.json?after=t3_9brpoh\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bqo5m\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bpwzh\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bneqs\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bpvjx\n",
      "https://www.reddit.com/r/depression.json?after=t3_9blfz2\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bqxi1\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bot2t\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bgufz\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bp9vh\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bmm3a\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bh3a5\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bikom\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bas4g\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bk872\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bj8yb\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bhguy\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bi4ha\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bglhj\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bhbsn\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bf6b3\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bcffi\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bfx9y\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bc17d\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bez7f\n",
      "https://www.reddit.com/r/depression.json?after=t3_9be7ec\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bdda5\n",
      "https://www.reddit.com/r/depression.json?after=t3_9b99zd\n",
      "https://www.reddit.com/r/depression.json?after=t3_9bb5s9\n",
      "https://www.reddit.com/r/depression.json?after=t3_9b7i6t\n",
      "https://www.reddit.com/r/depression.json?after=t3_9b92vd\n",
      "https://www.reddit.com/r/depression.json?after=t3_9b8dsn\n",
      "https://www.reddit.com/r/depression.json?after=t3_9b54f8\n",
      "https://www.reddit.com/r/depression.json?after=t3_9b0u4q\n"
     ]
    }
   ],
   "source": [
    "posts_depression = []\n",
    "after = None\n",
    "\n",
    "for i in range(40):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '?after=' + after\n",
    "    print(current_url)\n",
    "    res = requests.get(current_url, headers = {'User-agent': 'Naman 1.0'})\n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts_depression.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.reddit.com/r/depression.json?after=t3_9ajnm4', headers = {'User-agent': 'Naman 1.0'})\n",
    "current_dict = res.json()\n",
    "current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "posts_depression.extend(current_posts)\n",
    "after = current_dict['data']['after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(979, 979)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_depression), len(set([p['name'] for p in posts_depression]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/rant.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/rant.json\n",
      "https://www.reddit.com/r/rant.json?after=t3_9br5f6\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bu471\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bqbty\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bohwn\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bcavl\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bel76\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bfjqi\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bb5gu\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bdrqm\n",
      "https://www.reddit.com/r/rant.json?after=t3_9bbks7\n",
      "https://www.reddit.com/r/rant.json?after=t3_9b23xm\n",
      "https://www.reddit.com/r/rant.json?after=t3_9b2tup\n",
      "https://www.reddit.com/r/rant.json?after=t3_9b3bmk\n",
      "https://www.reddit.com/r/rant.json?after=t3_9atwq5\n",
      "https://www.reddit.com/r/rant.json?after=t3_9asyty\n",
      "https://www.reddit.com/r/rant.json?after=t3_9arlrj\n",
      "https://www.reddit.com/r/rant.json?after=t3_9aq64z\n",
      "https://www.reddit.com/r/rant.json?after=t3_9agi6y\n",
      "https://www.reddit.com/r/rant.json?after=t3_9abahw\n",
      "https://www.reddit.com/r/rant.json?after=t3_9aeu6f\n",
      "https://www.reddit.com/r/rant.json?after=t3_9ab1fr\n",
      "https://www.reddit.com/r/rant.json?after=t3_9a7qz6\n",
      "https://www.reddit.com/r/rant.json?after=t3_9a4j3f\n",
      "https://www.reddit.com/r/rant.json?after=t3_99x9bm\n",
      "https://www.reddit.com/r/rant.json?after=t3_99uzmk\n",
      "https://www.reddit.com/r/rant.json?after=t3_99s6mt\n",
      "https://www.reddit.com/r/rant.json?after=t3_99qwzf\n",
      "https://www.reddit.com/r/rant.json?after=t3_99qv1s\n",
      "https://www.reddit.com/r/rant.json?after=t3_99ks4c\n",
      "https://www.reddit.com/r/rant.json?after=t3_999tg5\n",
      "https://www.reddit.com/r/rant.json?after=t3_99bubm\n",
      "https://www.reddit.com/r/rant.json?after=t3_99bjaq\n",
      "https://www.reddit.com/r/rant.json?after=t3_997wa3\n",
      "https://www.reddit.com/r/rant.json?after=t3_992yh0\n",
      "https://www.reddit.com/r/rant.json?after=t3_98zefn\n",
      "https://www.reddit.com/r/rant.json?after=t3_98u173\n",
      "https://www.reddit.com/r/rant.json?after=t3_98x7lr\n",
      "https://www.reddit.com/r/rant.json?after=t3_98okqu\n",
      "https://www.reddit.com/r/rant.json?after=t3_98hxw6\n"
     ]
    }
   ],
   "source": [
    "posts_rant = []\n",
    "after = None\n",
    "\n",
    "for i in range(40):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '?after=' + after\n",
    "    print(current_url)\n",
    "    res = requests.get(current_url, headers = {'User-agent': 'Naman 1.0'})\n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts_rant.extend(current_posts)\n",
    "    after = current_dict['data']['after']\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 977)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_rant), len(set([p['name'] for p in posts_rant]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a JSON\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/depression.json', 'w+') as f:\n",
    "    json.dump(posts_depression, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/rant.json', 'w+') as f:\n",
    "    json.dump(posts_rant, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
